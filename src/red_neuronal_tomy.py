# -*- coding: utf-8 -*-
"""audio_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FSuhN9J_jBwbbEVAEdY7T3-XrKnMWH5o
"""

from google.colab import drive
drive.mount('/content/drive')



import pandas as pd
import numpy as np
import librosa
import librosa.display


from sklearn.preprocessing import MinMaxScaler, StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dropout
from sklearn.utils.class_weight import compute_class_weight

csv_path = "/content/drive/My Drive/Colab Notebooks/nuwehack-data-AI_SR/data/labels_paths_train.csv"

df = pd.read_csv(csv_path)



data = pd.read_csv("/content/drive/My Drive/Colab Notebooks/nuwehack-data-AI_SR/data/labels_paths_train.csv")


X_paths_sin_prefijo = data['Path'].tolist()
prefijo = "/content/drive/My Drive/Colab Notebooks/nuwehack-data-AI_SR/"
X_paths = [prefijo + path for path in X_paths_sin_prefijo]

y_labels = data['Label'].tolist()


def preprocess_audio(file_path):
    y, sr = librosa.load(file_path)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr)
    mel_spec = librosa.power_to_db(mel_spec, ref=np.max)
    return mel_spec

X_processed = [preprocess_audio(file_path) for file_path in X_paths]
X_processed = np.array(X_processed)
y = np.array(y_labels)
num_classes = len(set(y))
X_processed_expanded = np.expand_dims(X_processed, axis=-1)

print(X_processed_expanded.shape)

#scaler = MinMaxScaler()
scaler = StandardScaler()

X_reshaped = X_processed_expanded.reshape((700, -1))
X_scaled = scaler.fit_transform(X_reshaped)
X_scaled = X_scaled.reshape((700, 128, 216, 1))

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=24)

class_weights = compute_class_weight(
                                        class_weight = "balanced",
                                        classes = np.unique(y_train),
                                        y = y_train )
#class_weights = dict(zip(np.unique(y_train), class_weights))


model = models.Sequential([
    layers.Input(shape=(X_scaled.shape[1], X_scaled.shape[2])),
    layers.Conv1D(32, 3, activation='relu'),
    layers.MaxPooling1D(2),
    layers.Conv1D(64, 3, activation='relu'),
    layers.MaxPooling1D(2),
    layers.LSTM(128, return_sequences=True),
    layers.Dropout(0.15),
    layers.LSTM(64),
    layers.Dropout(0.15),
    layers.Dense(64, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history= model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1)#, class_weight=class_weights)

test_loss, test_acc = model.evaluate(X_test, y_test)
print('Test accuracy:', test_acc)


import joblib


joblib.dump(model, 'model.pkl')